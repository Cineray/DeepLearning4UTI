# DeepLearning4UTI
The aim of this repository is to create a comprehensive, curated list of resources which can be used for ultrasound tongue image (UTI) analysis, especially for the deep learning-based approaches.

## Contents

* [Scientific Paper](#scientific-papers)
* [Code](#other-resources)
* [Related Labs](#related-lists)
* [Contributing](#contributing)
* [License](#license)

## Scientific Papers
#### Motion Tracking
* [A guide to analysing tongue motion from ultrasound images.] (https://www.dental.umaryland.edu/media/sod/vocal-tract-visualization-laboratory/Guide_to_Ultrasound.pdf) Stone, Maureen. (2005) Clinical linguistics & phonetics.
* [Automatic contour tracking in ultrasound images.] (https://pdfs.semanticscholar.org/3f1a/2e2ccc5774a60b527ac6f5a7d0665b25895b.pdf) Li, Min, Chandra Kambhamettu, and Maureen Stone (2005) Clinical linguistics & phonetics.
* [Tongue contour extraction from ultrasound images based on deep neural network.](https://arxiv.org/ftp/arxiv/papers/1605/1605.05912.pdf) Jaumard-Hakoun, Aurore, et al. (2016) ICPhS.
* [A comparative study on the contour tracking algorithms in ultrasound tongue images with automatic re-initialization.](https://asa.scitation.org/doi/full/10.1121/1.4951024?TRACK=RSS) Xu, Kele, et al. (2016) The Journal of the Acoustical Society of America Express Letters.
* [Robust contour tracking in ultrasound tongue image sequences.](https://www.tandfonline.com/doi/abs/10.3109/02699206.2015.1110714) Xu, Kele, et al. (2016) Clinical linguistics & phonetics.


#### Other Analysis

## Code

* [Predicting tongue motion in unlabeled ultrasound videos using convolutional LSTM neural network](https://github.com/shuiliwanwu/ConvLstm-ultrasound-videos) -  Chaojie Zhao, Peng Zhang, Jian Zhu, Chengrui Wu, Huaimin Wang, Kele Xu. ICASSP 2019.

## Related Labs
* [Vocal Tract Visualization Laboratory, University of Maryland, Baltimore, USA](https://www.dental.umaryland.edu/speech/) 
* [Haskins Laboratories, Yale University, New Haven, USA](http://www.haskins.yale.edu/understandingspeech.html) 
* [Speech Disorders & Technology Lab, UT Dallas, USA](https://www.utdallas.edu/wanglab/) 
* [School of Electrical Engineering and Computer Science, University of Ottawa, Canada](http://www.site.uottawa.ca/~wslee/index.shtml) 
* [Laboratoire Signaux, Mod les, Apprentissage statistique (SIGMA), Paris, France](https://www.neurones.espci.fr/index_E.htm)
* [The Langevin Institute, Paris, France](https://www.institut-langevin.espci.fr/the_langevin_institute?lang=en)
* [GIPSA-lab, Grenoble, France](http://www.gipsa-lab.grenoble-inp.fr/en/home.php)
* [Centre for Speech Technology Research, University of Edinburgh, UK](http://www.cstr.ed.ac.uk/)
* [Psychological Sciences and Health, University of Strathclyde, UK](https://www.strath.ac.uk/humanities/psychologicalscienceshealth/)
* [Clinical Audiology, Speech and Language Research Centre, Queen Margaret University, UK](https://www.qmu.ac.uk/research-and-knowledge-exchange/research-centres-institutes-and-groups/clinical-audiology-speech-and-language-research-centre/)
* [Articulate Instruments Ltd., UK](www.articulateinstruments.com/)
* [Speech Technology and Smart Interactions Laboratory, Budapest University of Technology and Economics, Hungary](http://smartlab.tmit.bme.hu/index-en)
* [The Key Laboratory of Cognitive Computing and Applications, Tianjin University, China](http://cs.tju.edu.cn/csweben/researchdetail?item=KLCCA)


## Contributing

Your contributions are always welcome! Please take a look at the [contribution guidelines](CONTRIBUTING.md) first.

I will keep some pull requests open if I'm not sure whether those libraries are awesome, you could vote for them by adding üëç to them.

## License

[![License: CC BY 4.0](https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by/4.0/)
